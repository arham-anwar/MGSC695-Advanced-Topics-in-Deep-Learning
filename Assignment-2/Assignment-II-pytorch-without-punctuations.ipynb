{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment II - Text Generation with RNNs (Without Punctuations)\n",
    "Submitted by Arham Anwar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# seed and immports \n",
    "\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import random\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    pl.seed_everything(seed)\n",
    "\n",
    "# Setting the seed\n",
    "SEED = 42\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as: shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"tiny shakespeare dataset\"\"\"\n",
    "\n",
    "# Data preparation\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "filename = 'shakespeare.txt'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "print(f\"File downloaded and saved as: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Lower casing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the text\n",
    "text = open(filename, 'rb').read().decode(encoding='utf-8').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Using 500K characters only due to compute resource limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for compute restrictions we will use only 500000 characters\n",
    "text = text[100000:800000]\n",
    "\n",
    "# remove all punctuations and special characters\n",
    "text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "text = text.replace('!', '').replace('?', '').replace(';', '').replace(':', '').replace(',', '')\n",
    "text = text.replace('(', '').replace(')', '').replace('--', '').replace('?', '').replace('.', '')\n",
    "text = text.replace('\"', '').replace(\"'\", '').replace('_', '').replace('-', '').replace('`', '')\n",
    "text = text.replace('*', '').replace(']', '').replace('[', '').replace('}', '').replace('{', '')\n",
    "text = text.replace('1', '').replace('2', '').replace('3', '').replace('4', '').replace('5', '')\n",
    "text = text.replace('6', '').replace('7', '').replace('8', '').replace('9', '').replace('0', '')\n",
    "text = text.replace('=', '').replace('+', '').replace('<', '').replace('>', '').replace('/', '')\n",
    "text = text.replace('\\\\', '').replace('|', '').replace('@', '').replace('#', '').replace('$', '')\n",
    "text = text.replace('%', '').replace('^', '').replace('&', '').replace('*', '').replace('~', '')\n",
    "text = text.replace('`', '').replace('´', '').replace('§', '').replace('°', '').replace('¨', '')\n",
    "text = text.replace('£', '').replace('€', '').replace('¥', '').replace('¢', '').replace('¬', '')\n",
    "text = text.replace('µ', '').replace('¶', '').replace('©', '').replace('®', '').replace('™', '')\n",
    "\n",
    "\n",
    "#text = text[300000:800000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Character Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique characters\n",
    "characters = sorted(set(text))\n",
    "char_to_index = {c: i for i, c in enumerate(characters)}\n",
    "index_to_char = {i: c for i, c in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Sequence Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 40\n",
    "STEP_SIZE = 3\n",
    "sentences = []\n",
    "next_characters = []\n",
    "\n",
    "# Assuming 'text' and 'char_to_index' are defined earlier in your code\n",
    "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
    "    sentences.append(text[i: i + SEQ_LENGTH])\n",
    "    next_characters.append(text[i + SEQ_LENGTH])\n",
    "\n",
    "# Convert data to indices\n",
    "import numpy as np\n",
    "X = np.zeros((len(sentences), SEQ_LENGTH), dtype=np.int32)\n",
    "y = np.zeros((len(sentences)), dtype=np.int32)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    X[i] = [char_to_index[char] for char in sentence]\n",
    "    y[i] = char_to_index[next_characters[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Set a larger batch size\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "dataset = ShakespeareDataset(X, y)\n",
    "\n",
    "# split to train and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory checkpoints/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | lstm | LSTM   | 344 K \n",
      "1 | fc   | Linear | 3.5 K \n",
      "--------------------------------\n",
      "348 K     Trainable params\n",
      "0         Non-trainable params\n",
      "348 K     Total params\n",
      "1.392     Total estimated model params size (MB)\n",
      "2024-05-27 00:16:33.768777: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 00:16:47.206620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 869/869 [06:39<00:00,  2.18it/s, loss=2.21, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 2.175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 869/869 [06:39<00:00,  2.17it/s, loss=2.21, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 695: 'val_loss' reached 2.17461 (best 2.17461), saving model to 'checkpoints/shakespeare-epoch=00-val_loss=2.17.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 869/869 [06:29<00:00,  2.23it/s, loss=2.01, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.195 >= min_delta = 0.0. New best score: 1.979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 869/869 [06:30<00:00,  2.23it/s, loss=2.01, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1390: 'val_loss' reached 1.97925 (best 1.97925), saving model to 'checkpoints/shakespeare-epoch=01-val_loss=1.98.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 869/869 [06:42<00:00,  2.16it/s, loss=1.88, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.137 >= min_delta = 0.0. New best score: 1.842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 869/869 [06:43<00:00,  2.16it/s, loss=1.88, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 2085: 'val_loss' reached 1.84201 (best 1.84201), saving model to 'checkpoints/shakespeare-epoch=02-val_loss=1.84.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 869/869 [06:15<00:00,  2.31it/s, loss=1.78, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.096 >= min_delta = 0.0. New best score: 1.746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 869/869 [06:16<00:00,  2.31it/s, loss=1.78, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 2780: 'val_loss' reached 1.74581 (best 1.74581), saving model to 'checkpoints/shakespeare-epoch=03-val_loss=1.75.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 869/869 [06:28<00:00,  2.23it/s, loss=1.74, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.069 >= min_delta = 0.0. New best score: 1.677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 869/869 [06:29<00:00,  2.23it/s, loss=1.74, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 3475: 'val_loss' reached 1.67730 (best 1.67730), saving model to 'checkpoints/shakespeare-epoch=04-val_loss=1.68.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 869/869 [07:59<00:00,  1.81it/s, loss=1.67, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.044 >= min_delta = 0.0. New best score: 1.633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 869/869 [07:59<00:00,  1.81it/s, loss=1.67, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 4170: 'val_loss' reached 1.63346 (best 1.63346), saving model to 'checkpoints/shakespeare-epoch=05-val_loss=1.63.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 869/869 [07:59<00:00,  1.81it/s, loss=1.67, v_num=17]\n"
     ]
    }
   ],
   "source": [
    "class ShakespeareModel(pl.LightningModule):\n",
    "    def __init__(self, n_chars, hidden_size, num_layers, lr, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lstm = nn.LSTM(self.hparams.n_chars, self.hparams.hidden_size, self.hparams.num_layers, batch_first=True, dropout=self.hparams.dropout)\n",
    "        self.fc = nn.Linear(self.hparams.hidden_size, self.hparams.n_chars)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.one_hot(x, num_classes=self.hparams.n_chars).float()\n",
    "        h0 = torch.zeros(self.hparams.num_layers, x.size(0), self.hparams.hidden_size, device=self.device)\n",
    "        c0 = torch.zeros(self.hparams.num_layers, x.size(0), self.hparams.hidden_size, device=self.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = nn.CrossEntropyLoss()(y_hat, y) \n",
    "        self.log('val_loss', val_loss)\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        if outputs:\n",
    "            avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "            self.log('val_loss', avg_loss)\n",
    "        else:\n",
    "            self.log('val_loss', torch.tensor(float('nan')))\n",
    "            print(\"Warning: No validation outputs were generated. Check your data.\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def generate_text(self, seed_text, max_length=100, temperature=1.0):\n",
    "        self.eval()\n",
    "        generated_text = seed_text\n",
    "        input_ids = torch.tensor([char_to_index[c] for c in seed_text], dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length - len(seed_text)):\n",
    "                logits = self(input_ids)\n",
    "                logits = logits[0, :] / temperature  # Apply temperature correctly to logits\n",
    "                probabilities = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "                predicted_char_index = torch.multinomial(probabilities, 1).item()  # Sample from the distribution\n",
    "                predicted_char = index_to_char[predicted_char_index]\n",
    "                generated_text += predicted_char\n",
    "                next_input = torch.tensor([[predicted_char_index]], dtype=torch.long).to(self.device)\n",
    "                input_ids = torch.cat([input_ids[:, 1:], next_input], dim=1)  # Shift and append\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "# Instantiate the model with recommended hyperparameters\n",
    "n_chars = len(char_to_index)  # Ensure this is set based on your dataset\n",
    "hidden_size = 128  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "lr = 0.001  # Reduced learning rate\n",
    "dropout = 0.3  # Added dropout for regularization\n",
    "\n",
    "model = ShakespeareModel(n_chars, hidden_size, num_layers, lr, dropout)\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='shakespeare-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    save_weights_only=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"shakespeare_model\")\n",
    "\n",
    "# Define the Trainer with the checkpoint and early stopping callbacks\n",
    "trainer = Trainer(\n",
    "    max_epochs=6,\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Assuming train_dataloader and val_dataloader are defined\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is the world the see the will the world the see the rese and the porith thou shall the come the warwick the for the come the world i would not the will the good the\n"
     ]
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=200, temperature=0.2)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is i with the was\n",
      "the farewell courten in the day than there to he see thee\n",
      "for the warwick be i to the dount then the for the\n",
      "will the say for shall richard in the that or be the\n",
      "geart to thus see the ban should the come the serviles do\n",
      "and the prove in the to anther moster and well the soul\n",
      "him the since a come in the comes king henry with the\n",
      "parse i with are to fight that i well be the son\n",
      "her the fear be on the warwond and the garis and with\n",
      "the dead father the death the sware be such that the good\n"
     ]
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=1000, temperature=0.5)\n",
    "# print such that text is new line after 12 words each\n",
    "print(' '.join(generated_text.split()[:12]))\n",
    "print(' '.join(generated_text.split()[12:24]))\n",
    "print(' '.join(generated_text.split()[24:36]))\n",
    "print(' '.join(generated_text.split()[36:48]))\n",
    "print(' '.join(generated_text.split()[48:60]))\n",
    "print(' '.join(generated_text.split()[60:72]))\n",
    "print(' '.join(generated_text.split()[72:84]))\n",
    "print(' '.join(generated_text.split()[84:96]))\n",
    "print(' '.join(generated_text.split()[96:108]))\n",
    "print(' '.join(generated_text.split()[108:120]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is the seat the see\n",
      "the such my lord the world what the for the see the\n",
      "death the will shall the heart the good the will the present\n",
      "the death the come the say the will the soul the see\n",
      "the soul my lord and the were the word the see the\n",
      "will so the soul the soul the more the warwick the lord\n",
      "in the see the see the world the see the warwick the\n",
      "grom the soul the god the wall the deep the world with\n",
      "the see the death the prese the bear the see the world\n",
      "the soul the soul the will the counter the beather the fore\n"
     ]
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=1000, temperature=0.2)\n",
    "# print such that text is new line after 12 words each\n",
    "print(' '.join(generated_text.split()[:12]))\n",
    "print(' '.join(generated_text.split()[12:24]))\n",
    "print(' '.join(generated_text.split()[24:36]))\n",
    "print(' '.join(generated_text.split()[36:48]))\n",
    "print(' '.join(generated_text.split()[48:60]))\n",
    "print(' '.join(generated_text.split()[60:72]))\n",
    "print(' '.join(generated_text.split()[72:84]))\n",
    "print(' '.join(generated_text.split()[84:96]))\n",
    "print(' '.join(generated_text.split()[96:108]))\n",
    "print(' '.join(generated_text.split()[108:120]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is this seet the say\n",
      "and the count the king and see my lord and the warwick\n",
      "the bead the the pear the prine the world the courter the\n",
      "bear my lord and sir thee and the earth the say the\n",
      "man be the world be the dake the conter son well the\n",
      "dead the rebored is the cate in the death the seat the\n",
      "warwick the recest the hast of the hast the prother the pore\n",
      "the dosh with the will come the death the seat the warwick\n",
      "i the nother so be the ant the dead the court that\n",
      "your be the sound the warwick the sepore in he pray the\n",
      "not see the for i would nor the will houd hath the\n",
      "gaunter and the grow me the death of the come the and\n",
      "the rest the warwick with the world of more king i me\n",
      "the see the say the for the will the had the duke\n",
      "the house and so the prince and with i tear i the\n",
      "man a two beath god the was this i be the can\n",
      "it you are the prese the noble us me that and shall\n",
      "not the make the beather my come not see there the sent\n",
      "the bear the service and my l\n"
     ]
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=1000, temperature=0.4)\n",
    "\n",
    "def print_text_in_chunks(text, chunk_size=12):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        print(' '.join(words[i:i + chunk_size]))\n",
    "\n",
    "print_text_in_chunks(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is for the too shall\n",
      "spould romeo grutt i death the king then our we plant and\n",
      "the ban peine look i for hath more the world thou my\n",
      "roke my lord king recordy that me for the come shall sole\n",
      "i will that and the have make as my lord the seat\n",
      "the seak the nent he down the farese the bingh the made\n",
      "the some as nom it thy cail not more were what the\n",
      "for a honour the do not staul with his parse duchess be\n",
      "he life the soul seal be the bear the not the neth\n",
      "your spould the fore the count in then seal i do the\n",
      "courtens but thou the sell thou have tear i leave and on\n",
      "our mont bound to they with some the lear thus dear the\n",
      "romeo shall why so me the come and king and him that\n",
      "is the some the beath and where that word is be the\n",
      "were the exent the engole and well see i wend the deesing\n",
      "to me more me i am may the canunt the seat the\n",
      "brink the were a come at this preven there the dool thou\n",
      "art the want and the deep then must so the grace the\n",
      "lord go and what seat\n"
     ]
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=1000, temperature=0.6)\n",
    "\n",
    "def print_text_in_chunks(text, chunk_size=12):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        print(' '.join(words[i:i + chunk_size]))\n",
    "\n",
    "print_text_in_chunks(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is your senving and seeven\n",
      "my saul not my lord i foul my would a kind them\n",
      "fathers a pourt of there to that here what margo me i\n",
      "wail way and the sike this the lorded parst the will shall\n",
      "to the the counton have teseess gloucester amay mone thou such pronn\n",
      "and bine draw that not then have save and who thee seatel\n",
      "of your and have on to tweo shall to it edward is\n",
      "therefore comnot be feor shall lead leaveming te heart nor which thi\n",
      "see the backoring of with me i come of aglord that speak\n",
      "chate bes iw not mother i come i foot such richaldbing he\n",
      "may iut thou art eovinglan toight i were all that but edward\n",
      "now her with see their frole the gester be theme mard and\n",
      "for love of yorks soul some dast not wour head be enfere\n",
      "god and than then seed this breath is gonour shall a good\n",
      "by mach is he well come that siin more a bester to\n",
      "the shall frether and now been to make be though moue that\n",
      "how this like good and that the romeo and the our so\n",
      "lord uram the\n"
     ]
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=1000, temperature=0.8)\n",
    "\n",
    "def print_text_in_chunks(text, chunk_size=12):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        print(' '.join(words[i:i + chunk_size]))\n",
    "\n",
    "print_text_in_chunks(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is me hen thou the\n",
      "boy fasces to geword shole puaring his mein supand flow i poor\n",
      "themh word of galing shall engle prinn of she eay her again\n",
      "time i would pref and bourse thing hath to crevack ge plkiend\n",
      "you ridester comvers on mown vave before to therefore beages the and\n",
      "frem sour seal he seallw pestering all thee werly i low not\n",
      "me preskd of wesniot to yes see shall slave boonage to us\n",
      "out sise is me fall stare cherelitn thou bolingbrokes we than is\n",
      "nok weir my all beling of lord be your as should piges\n",
      "rest hin heavy stardd king store in fur hath for midel is\n",
      "sead hid make twem theme which king will the tear with lords\n",
      "guapts of the here sufs i pefore truids than the right not\n",
      "your be are death the queen to thou beather thou me with\n",
      "me no remeid o rasp vise thus my list saiter to your\n",
      "god wyrfas be nobe of say delils land is sucs and i\n",
      "wan newer that the ris i how prieis then i wie theres\n",
      "with thee for myter wo moul treas is too thou pronath\n"
     ]
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # change seed here\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=1000, temperature=1.0)\n",
    "\n",
    "def print_text_in_chunks(text, chunk_size=12):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        print(' '.join(words[i:i + chunk_size]))\n",
    "\n",
    "print_text_in_chunks(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is your no the fakeily\n",
      "on my mald vitle upfer my to nor onluy and no recommy\n",
      "is now sleep never my gave a durn ord thee edwleds my\n",
      "duch i cree before comeson a cameland be suit un enour kand\n",
      "hele fear and she jess thy god strake a sert it a\n",
      "man he aruthine me then foor ale a with hereful stould her\n",
      "in forghat kole while gloucester that mesters i hnop this to speaks\n",
      "our some preatan o doth thee apal appirst nor mowent thou gongh\n",
      "whis samen to stoucome and verooy with his world aldfppen a were\n",
      "men to wity ged may i mauntt would loves gast he tell\n"
     ]
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=1000, temperature=1)\n",
    "# print such that text is new line after 12 words each\n",
    "print(' '.join(generated_text.split()[:12]))\n",
    "print(' '.join(generated_text.split()[12:24]))\n",
    "print(' '.join(generated_text.split()[24:36]))\n",
    "print(' '.join(generated_text.split()[36:48]))\n",
    "print(' '.join(generated_text.split()[48:60]))\n",
    "print(' '.join(generated_text.split()[60:72]))\n",
    "print(' '.join(generated_text.split()[72:84]))\n",
    "print(' '.join(generated_text.split()[84:96]))\n",
    "print(' '.join(generated_text.split()[96:108]))\n",
    "print(' '.join(generated_text.split()[108:120]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
