{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment II - Text Generation with RNNs\n",
    "Submitted by Arham Anwar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# seed and immports \n",
    "\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    pl.seed_everything(seed)\n",
    "\n",
    "# Setting the seed\n",
    "SEED = 42\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as: shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"tiny shakespeare dataset\"\"\"\n",
    "\n",
    "# Data preparation\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "filename = 'shakespeare.txt'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "print(f\"File downloaded and saved as: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Lower casing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the text\n",
    "text = open(filename, 'rb').read().decode(encoding='utf-8').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Using 500K characters only due to compute resource limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for compute restrictions we will use only 500000 characters\n",
    "text = text[300000:800000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Character Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique characters\n",
    "characters = sorted(set(text))\n",
    "char_to_index = {c: i for i, c in enumerate(characters)}\n",
    "index_to_char = {i: c for i, c in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Sequence Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEQ_LENGTH = 40\n",
    "STEP_SIZE = 3\n",
    "sentences = []\n",
    "next_characters = []\n",
    "\n",
    "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
    "    sentences.append(text[i: i + SEQ_LENGTH])\n",
    "    next_characters.append(text[i + SEQ_LENGTH])\n",
    "\n",
    "# Convert data to indices\n",
    "X = np.zeros((len(sentences), SEQ_LENGTH), dtype=np.int32)\n",
    "y = np.zeros((len(sentences)), dtype=np.int32)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    X[i] = [char_to_index[char] for char in sentence]\n",
    "    y[i] = char_to_index[next_characters[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Set a larger batch size\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "dataset = ShakespeareDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | lstm | LSTM   | 218 K \n",
      "1 | fc   | Linear | 5.0 K \n",
      "--------------------------------\n",
      "223 K     Trainable params\n",
      "0         Non-trainable params\n",
      "223 K     Total params\n",
      "0.895     Total estimated model params size (MB)\n",
      "2024-05-26 04:34:42.701722: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-26 04:34:48.088598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  14%|█▍        | 91/651 [00:26<02:44,  3.40it/s, loss=3, v_num=19]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "class ShakespeareModel(pl.LightningModule):\n",
    "    def __init__(self, n_chars, hidden_size, num_layers, lr):\n",
    "        super(ShakespeareModel, self).__init__()\n",
    "        self.n_chars = n_chars\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "\n",
    "        self.lstm = nn.LSTM(n_chars, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_chars)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.one_hot(x, num_classes=self.n_chars).float()\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def generate_text(self, seed_text, gen_length, temperature=1.0):\n",
    "        self.eval()\n",
    "        generated = seed_text\n",
    "        for _ in range(gen_length):\n",
    "            x_pred = torch.tensor([[char_to_index[char] for char in seed_text]], dtype=torch.long)\n",
    "            y_pred = self(x_pred.to(self.device)).squeeze()\n",
    "            y_pred = y_pred / temperature\n",
    "            probabilities = torch.nn.functional.softmax(y_pred, dim=-1).detach().cpu().numpy()\n",
    "            next_index = np.random.choice(len(characters), p=probabilities)\n",
    "            next_char = index_to_char[next_index]\n",
    "            generated += next_char\n",
    "            seed_text = seed_text[1:] + next_char\n",
    "        return generated\n",
    "\n",
    "# Instantiate the model\n",
    "n_chars = len(characters)\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "lr = 0.005\n",
    "model = ShakespeareModel(n_chars, hidden_size, num_layers, lr)\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(max_epochs=4, gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.fit(model, dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text geneeration outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using the trained model\n",
    "seed_text = \"to be or not to be that is the question\"\n",
    "generated_text = model.generate_text(seed_text, gen_length=500, temperature=0.8)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning is very deep but is it deep enough.\n",
      "\n",
      "romeo:\n",
      "now a king hope!\n",
      "\n",
      "bloyy:\n",
      "what you near things a since in prince, whom it is accain.\n",
      "\n",
      "king henry vi:\n",
      "and that see, not afford in high have that shall live,\n",
      "and give my horse to perhence of flored you,\n",
      "so innote what thou come to me in him to unlast,\n",
      "that and him, his rount, and it me arsent-blood;\n",
      "have upon cloaned, his hand: i'll come you scorn\n",
      "noight in his for both follow: and things and pity,\n",
      "for me it be to all herself\n",
      "which shall not the best heart nothing netter and linger.\n",
      "\n",
      "rome\n"
     ]
    }
   ],
   "source": [
    "# Generate text using the trained model\n",
    "seed_text = \"deep learning is very deep but is it deep enough\"\n",
    "generated_text = model.generate_text(seed_text, gen_length=500, temperature=0.8)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using the trained model\n",
    "seed_text = \"deep learning is very deep but is it deep enough\"\n",
    "generated_text = model.generate_text(seed_text, gen_length=500, temperature=0.2)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using the trained model\n",
    "seed_text = \"deep learning is very deep but is it deep enough\"\n",
    "generated_text = model.generate_text(seed_text, gen_length=500, temperature=0.4)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using the trained model\n",
    "seed_text = \"deep learning is very deep but is it deep enough\"\n",
    "generated_text = model.generate_text(seed_text, gen_length=500, temperature=0.6)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
