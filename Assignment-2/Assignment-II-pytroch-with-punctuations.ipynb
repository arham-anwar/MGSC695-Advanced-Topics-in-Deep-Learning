{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment II - Text Generation with RNNs\n",
    "Submitted by Arham Anwar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# seed and immports \n",
    "\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import random\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    pl.seed_everything(seed)\n",
    "\n",
    "# Setting the seed\n",
    "SEED = 42\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as: shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"tiny shakespeare dataset\"\"\"\n",
    "\n",
    "# Data preparation\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "filename = 'shakespeare.txt'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "print(f\"File downloaded and saved as: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Lower casing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the text\n",
    "text = open(filename, 'rb').read().decode(encoding='utf-8').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Using 500K characters only due to compute resource limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for compute restrictions we will use only 500000 characters\n",
    "text = text[100000:800000]\n",
    "#text = text[300000:800000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Character Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique characters\n",
    "characters = sorted(set(text))\n",
    "char_to_index = {c: i for i, c in enumerate(characters)}\n",
    "index_to_char = {i: c for i, c in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Sequence Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 40\n",
    "STEP_SIZE = 3\n",
    "sentences = []\n",
    "next_characters = []\n",
    "\n",
    "# Assuming 'text' and 'char_to_index' are defined earlier in your code\n",
    "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
    "    sentences.append(text[i: i + SEQ_LENGTH])\n",
    "    next_characters.append(text[i + SEQ_LENGTH])\n",
    "\n",
    "# Convert data to indices\n",
    "import numpy as np\n",
    "X = np.zeros((len(sentences), SEQ_LENGTH), dtype=np.int32)\n",
    "y = np.zeros((len(sentences)), dtype=np.int32)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    X[i] = [char_to_index[char] for char in sentence]\n",
    "    y[i] = char_to_index[next_characters[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Set a larger batch size\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "dataset = ShakespeareDataset(X, y)\n",
    "\n",
    "# split to train and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | lstm | LSTM   | 350 K \n",
      "1 | fc   | Linear | 5.0 K \n",
      "--------------------------------\n",
      "355 K     Trainable params\n",
      "0         Non-trainable params\n",
      "355 K     Total params\n",
      "1.423     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|▋         | 43/652 [13:35<3:12:31, 18.97s/it, loss=3.08, v_num=14]\n",
      "Epoch 0: 100%|██████████| 913/913 [06:30<00:00,  2.34it/s, loss=2.35, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 2.350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 913/913 [06:30<00:00,  2.34it/s, loss=2.35, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 730: 'val_loss' reached 2.34978 (best 2.34978), saving model to 'checkpoints/shakespeare-epoch=00-val_loss=2.35.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 913/913 [05:29<00:00,  2.77it/s, loss=2.16, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.189 >= min_delta = 0.0. New best score: 2.161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 913/913 [05:29<00:00,  2.77it/s, loss=2.16, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1460: 'val_loss' reached 2.16053 (best 2.16053), saving model to 'checkpoints/shakespeare-epoch=01-val_loss=2.16.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 913/913 [05:27<00:00,  2.79it/s, loss=2.07, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.136 >= min_delta = 0.0. New best score: 2.024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 913/913 [05:27<00:00,  2.79it/s, loss=2.07, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 2190: 'val_loss' reached 2.02431 (best 2.02431), saving model to 'checkpoints/shakespeare-epoch=02-val_loss=2.02.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 913/913 [05:32<00:00,  2.74it/s, loss=1.98, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.117 >= min_delta = 0.0. New best score: 1.907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 913/913 [05:32<00:00,  2.74it/s, loss=1.98, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 2920: 'val_loss' reached 1.90748 (best 1.90748), saving model to 'checkpoints/shakespeare-epoch=03-val_loss=1.91.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 913/913 [06:46<00:00,  2.24it/s, loss=1.88, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.074 >= min_delta = 0.0. New best score: 1.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 913/913 [06:46<00:00,  2.24it/s, loss=1.88, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 3650: 'val_loss' reached 1.83333 (best 1.83333), saving model to 'checkpoints/shakespeare-epoch=04-val_loss=1.83.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 913/913 [06:20<00:00,  2.40it/s, loss=1.8, v_num=16] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.070 >= min_delta = 0.0. New best score: 1.764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 913/913 [06:20<00:00,  2.40it/s, loss=1.8, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 4380: 'val_loss' reached 1.76364 (best 1.76364), saving model to 'checkpoints/shakespeare-epoch=05-val_loss=1.76.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 913/913 [06:20<00:00,  2.40it/s, loss=1.8, v_num=16]\n"
     ]
    }
   ],
   "source": [
    "class ShakespeareModel(pl.LightningModule):\n",
    "    def __init__(self, n_chars, hidden_size, num_layers, lr, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lstm = nn.LSTM(self.hparams.n_chars, self.hparams.hidden_size, self.hparams.num_layers, batch_first=True, dropout=self.hparams.dropout)\n",
    "        self.fc = nn.Linear(self.hparams.hidden_size, self.hparams.n_chars)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.one_hot(x, num_classes=self.hparams.n_chars).float()\n",
    "        h0 = torch.zeros(self.hparams.num_layers, x.size(0), self.hparams.hidden_size, device=self.device)\n",
    "        c0 = torch.zeros(self.hparams.num_layers, x.size(0), self.hparams.hidden_size, device=self.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = nn.CrossEntropyLoss()(y_hat, y) \n",
    "        self.log('val_loss', val_loss)\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        if outputs:\n",
    "            avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "            self.log('val_loss', avg_loss)\n",
    "        else:\n",
    "            self.log('val_loss', torch.tensor(float('nan')))\n",
    "            print(\"Warning: No validation outputs were generated. Check your data.\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def generate_text(self, seed_text, max_length=100, temperature=1.0):\n",
    "        self.eval()\n",
    "        generated_text = seed_text\n",
    "        input_ids = torch.tensor([char_to_index[c] for c in seed_text], dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length - len(seed_text)):\n",
    "                logits = self(input_ids)\n",
    "                logits = logits[0, :] / temperature  # Apply temperature correctly to logits\n",
    "                probabilities = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "                predicted_char_index = torch.multinomial(probabilities, 1).item()  # Sample from the distribution\n",
    "                predicted_char = index_to_char[predicted_char_index]\n",
    "                generated_text += predicted_char\n",
    "                next_input = torch.tensor([[predicted_char_index]], dtype=torch.long).to(self.device)\n",
    "                input_ids = torch.cat([input_ids[:, 1:], next_input], dim=1)  # Shift and append\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "# Instantiate the model with recommended hyperparameters\n",
    "n_chars = len(char_to_index)  # Ensure this is set based on your dataset\n",
    "hidden_size = 128  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "lr = 0.001  # Reduced learning rate\n",
    "dropout = 0.3  # Added dropout for regularization\n",
    "\n",
    "model = ShakespeareModel(n_chars, hidden_size, num_layers, lr, dropout)\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='shakespeare-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    save_weights_only=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"shakespeare_model\")\n",
    "\n",
    "# Define the Trainer with the checkpoint and early stopping callbacks\n",
    "trainer = Trainer(\n",
    "    max_epochs=6,\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Assuming train_dataloader and val_dataloader are defined\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is the will the some\n",
      "the thou the shord the stall the shall the stall the senter:\n",
      "the mard the have the with the come the bord.\n",
      "\n",
      "canilice:\n",
      "the some the will the wit\n"
     ]
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=200, temperature=0.2)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is the come\n",
      "the shall the come the shall the shord,\n",
      "and the sace the will the shall the forth and the best the stord\n",
      "the dichard the did the shall the shall the sha\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with recommended hyperparameters\n",
    "n_chars = 39  # Ensure this is set based on your dataset\n",
    "hidden_size = 128  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "lr = 0.001  # Reduced learning rate\n",
    "dropout = 0.3  # Added dropout for regularization\n",
    "\n",
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=200, temperature=0.2)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stability:\n",
      "the fares the done the come:\n",
      "and the will be the world be the consent the death,\n",
      "the shall the words the shall the shall the done the with the shord the present and the dichard the bectord \n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with recommended hyperparameters\n",
    "n_chars = 39  # Ensure this is set based on your dataset\n",
    "hidden_size = 128  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "lr = 0.001  # Reduced learning rate\n",
    "dropout = 0.3  # Added dropout for regularization\n",
    "\n",
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"stability\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=200, temperature=0.2)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is you\n",
      "have and my edward the pare the to the king the wird\n",
      "the dome and the have lath the ward,\n",
      "and the come the from the court thee with the jole,\n",
      "and the will be\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with recommended hyperparameters\n",
    "n_chars = 39  # Ensure this is set based on your dataset\n",
    "hidden_size = 128  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "lr = 0.001  # Reduced learning rate\n",
    "dropout = 0.3  # Added dropout for regularization\n",
    "\n",
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=200, temperature=0.4)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is the lirg dather\n",
      "where-to my what the werper thy sheard.\n",
      "thy court this not the wilt the bunker.\n",
      "\n",
      "king rothard:\n",
      "i bust the sharl my than do and compont,\n",
      "and the l\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with recommended hyperparameters\n",
    "n_chars = 39  # Ensure this is set based on your dataset\n",
    "hidden_size = 128  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "lr = 0.001  # Reduced learning rate\n",
    "dropout = 0.3  # Added dropout for regularization\n",
    "\n",
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=200, temperature=0.6)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is his his\n",
      "thought inseld with my, destace in the beers me so heride\n",
      "stere; the still to mam blow a dike,\n",
      "sweee rave the rards thi greve the hop\n",
      "say thee to the for\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with recommended hyperparameters\n",
    "n_chars = 39  # Ensure this is set based on your dataset\n",
    "hidden_size = 128  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "lr = 0.001  # Reduced learning rate\n",
    "dropout = 0.3  # Added dropout for regularization\n",
    "\n",
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=200, temperature=0.8)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is deep learning deep enough or not is it of's,\n",
      "and bight op you, so live, i not the burth:\n",
      "you!\n",
      "\n",
      "delove:\n",
      "nhor lomcy of ey: is our powers is that dir,\n",
      "beave usvan your leds, and ut him:\n",
      "is tere qreen on the gented:\n",
      "when thy; swould will that their better:\n",
      "iner unthink sild for desty plices,\n",
      "apon he dues combloutss his mikalt hen's os witing:\n",
      "and fool be this the for the picnte, mean my.\n",
      "-\n",
      "gry padiced: it reapherer burd, preastery.\n",
      "\n",
      "lincy ud he one:\n",
      "a stake carse: whree 'ting noight:\n",
      "the pursquet for elcked leqvors, but a twriens.\n",
      "-radsiens nebty amour nand sowled fortay\n",
      "to wind for the briy burthour is aphers\n",
      "the mich so digtther tong aalks lewist.\n",
      "\n",
      "mroriul:\n",
      "o's on stome shenp sharl, the lady,\n",
      "my homan, for soutunh some shy whow earth?\n",
      "\n",
      "gaist:\n",
      "i hame?\n",
      "and i'parculidferp that pray.\n",
      "bewlowl and bretchand the jeging deigts.\n",
      "hoth!\n",
      "\n",
      "kheng man\n",
      "morsed:\n",
      "the daven all you that no and me bege.\n",
      "betirienes with to thass saed, the shurg,\n",
      "my lord, beel of will that i eesons erest,\n",
      "and will to thy en\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with recommended hyperparameters\n",
    "n_chars = 39  # Ensure this is set based on your dataset\n",
    "hidden_size = 128  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "lr = 0.001  # Reduced learning rate\n",
    "dropout = 0.3  # Added dropout for regularization\n",
    "\n",
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ShakespeareModel.load_from_checkpoint(best_model_path, n_chars=n_chars, hidden_size=hidden_size, num_layers=num_layers, lr=lr, dropout=dropout)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Is deep learning deep enough or not is\"  # You can start with any seed text\n",
    "generated_text = model.generate_text(seed_text.lower(), max_length=1000, temperature=1)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
